#!/usr/bin/env python
# coding: utf-8

# # Проект: "Выбор локации для скважины"

# **Описание проекта:**
# 
# Вы работаете в добывающей компании **«ГлавРосГосНефть»**. Нужно решить, где бурить новую скважину. 
# 
# Шаги для выбора локации обычно такие:
# 1. В избранном регионе собирают характеристики для скважин: качество нефти и объём её запасов;
# 2. Строят модель для предсказания объёма запасов в новых скважинах;
# 3. Выбирают скважины с самыми высокими оценками значений;
# 4. Определяют регион с максимальной суммарной прибылью отобранных скважин.
# 
# Вам предоставлены пробы нефти в трёх регионах. Характеристики для каждой скважины в регионе также уже известны. 

# **Цель проекта:**
# 1. Построить модель для определения региона, где добыча принесёт наибольшую прибыль.
# 2. Проанализировать возможную прибыль и риски техникой Bootstrap.

# **Условия задачи:**
# 
# - Для обучения модели подходит только линейная регрессия **(остальные — недостаточно предсказуемые)**.
# - При разведке региона исследуют 500 точек, из которых с помощью машинного обучения выбирают 200 лучших для разработки.
# - Бюджет на разработку скважин в регионе — 10 млрд. рублей.
# - При нынешних ценах один баррель сырья приносит 450 рублей дохода. Доход с каждой единицы продукта составляет 450 тыс. рублей, поскольку объём указан в тысячах баррелей.
# - После оценки рисков нужно оставить лишь те регионы, в которых вероятность убытков меньше 2.5%. Среди них выбирают регион с наибольшей средней прибылью.
# 
# Данные синтетические: детали контрактов и характеристики месторождений не разглашаются.

# **Описание данных:**
# 
# Данные геологоразведки трёх регионов находятся в следующих файлах: *'/datasets/geo_data_0.csv', '/datasets/geo_data_1.csv', '/datasets/geo_data_2.csv'.* 
# 
# Для них:
# - id — уникальный идентификатор скважины;
# - f0, f1, f2 — три признака точек (неважно, что они означают, но сами признаки значимы);
# - product — объём запасов в скважине (тыс. баррелей).

# **План работы с проектом:**
# 1. Загрузка и изучение данных;
# 2. Предобработка данных;
# 3. Исследовательский анализ данных;
# 4. Проведение корреляционного анализа данных;
# 5. Обучение и проверка модели;
# 6. Подготовка к расчету прибыли;
# 7. Расчет прибыли и рисков;
# 8. Формирование итоговых выводов.

# ## Загрузка и изучение данных

# In[1]:


# загрузим стандартные библиотеки, необходимые для работы
import pandas as pd
import math
import sklearn
import numpy as np
from scipy import stats as st

# загрузим библиотеки для визуализации данных
import matplotlib.pyplot as plt 
import seaborn as sns

# загрузим модули, необходимые для работы
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

import warnings
warnings.filterwarnings("ignore")


# In[2]:


# введение констант
RANDOM_STATE = 42
N = 500 # количество исследуемых точек
K = 200 # количество точек для разработки
BUDGET = 10000000000 # бюджет на разработку скважин в регионе
BAR_COST = 450000 # доход с 1000 баррелей


# ### Загрузка и изучение данных файла "geo_data_0.csv"

# In[3]:


# загрузим данные из файла 'geo_data_0.csv'
try:
    # локальный путь к файлу
    data_0 = pd.read_csv('C:/Users/Admin/OneDrive/Рабочий стол/ПФ/МО_в_бизнесе/geo_data_0.csv')
except:
    # путь к файлу в тренажере
    data_0 = pd.read_csv('/datasets/geo_data_0.csv')

# посмотрим первые 5 строк датафрема
data_0.head()


# In[4]:


# посмотрим общую информацию о данных
data_0.info()


# In[5]:


# посмотрим описательную статистику данных
data_0.describe()


# **Примечание:** По полученным результатам видно, что данные соответствуют описанию задачи, типы данных в столбцах корректные. Пропуски в данных отсутствуют.

# ### Загрузка и изучение данных файла "geo_data_1.csv"

# In[6]:


# загрузим данные из файла 'geo_data_1.csv'
try:
    # локальный путь к файлу
    data_1 = pd.read_csv('C:/Users/Admin/OneDrive/Рабочий стол/ПФ/МО_в_бизнесе/geo_data_1.csv')
except:
    # путь к файлу в тренажере
    data_1 = pd.read_csv('/datasets/geo_data_1.csv')

# посмотрим первые 5 строк датафрема
data_1.head()


# In[7]:


# посмотрим общую информацию о данных
data_1.info()


# In[8]:


# посмотрим описательную статистику данных
data_1.describe()


# **Примечание:** По полученным результатам видно, что данные соответствуют описанию задачи, типы данных в столбцах корректные. Пропуски в данных отсутствуют.

# ### Загрузка и изучение данных файла "geo_data_2.csv"

# In[9]:


# загрузим данные из файла 'geo_data_2.csv'
try:
    # локальный путь к файлу
    data_2 = pd.read_csv('C:/Users/Admin/OneDrive/Рабочий стол/ПФ/МО_в_бизнесе/geo_data_2.csv')
except:
    # путь к файлу в тренажере
    data_2 = pd.read_csv('/datasets/geo_data_2.csv')

# посмотрим первые 5 строк датафрема
data_2.head()


# In[10]:


# посмотрим общую информацию о данных
data_2.info()


# In[11]:


# посмотрим описательную статистику данных
data_2.describe()


# **Примечание:** По полученным результатам видно, что данные соответствуют описанию задачи, типы данных в столбцах корректные. Пропуски в данных отсутствуют.

# **Обобщающий вывод этапа "Загрузка и изучение данных":** 
# 1. Данные во всех таблицах соответсвуют описанию задачи.
# 2. Пропуски во всех таблицах отсутствуют.
# 3. Типы данных для столбцов каждой из таблиц корректные.

# ## Предобработка данных

# ### Предобработка данных в таблице "data_0"

# In[12]:


# посмотрим количество пропусков в данных
data_0.isna().sum()


# **Примечание:** Как и было замечено ранее, пропуски в данных отсутствуют.

# In[13]:


# проверим датафрейм на наличие явных дубликатов
data_0.duplicated().sum()


# **Примечание:** Явные дубликаты в данных отсутствуют.

# In[14]:


# проверим датафрейм на наличие неявных дубликатов
# проверка будет проводиться по столбцу 'id'
data_0.duplicated(subset='id').sum()


# In[15]:


# проверим датафрейм на наличие неявных дубликатов
# проверка будет проводиться по столбцу 'id'
dupl_data_0 = data_0[data_0.duplicated(subset='id', keep=False)]  # keep=False помечает все дубликаты, включая их первое вхождение
dupl_data_0_sorted = dupl_data_0.sort_values('id')

# выведем полученные неявные дубликаты для анализа
print(f"Всего строк с дубликатами в столбце 'id': {len(dupl_data_0)}")
print("Строки с дубликатами:")
print(dupl_data_0_sorted)


# **Примечание:** Как показала проверка, явные дубликаты отсутствуют. Однако встречаются скважины с одинаковыым значением в столбце 'id', но разными значениями в других столбцах, скорее всего, это может быть связано с ошибкой присвоения уникального индентификатора скважен. Данные с одинаковыми значениями в столбце 'id' удалять не будем.

# ### Предобработка данных в таблице "data_1"

# In[16]:


# посмотрим количество пропусков в данных
data_1.isna().sum()


# **Примечание:** Как и было замечено ранее, пропуски в данных отсутствуют.

# In[17]:


# проверим датафрейм на наличие явных дубликатов
data_1.duplicated().sum()


# **Примечание:** Явные дубликаты в данных отсутствуют.

# In[18]:


# проверим датафрейм на наличие неявных дубликатов
# проверка будет проводиться по столбцу 'id'
data_1.duplicated(subset='id').sum()


# In[19]:


# проверим датафрейм на наличие неявных дубликатов
# проверка будет проводиться по столбцу 'id'
dupl_data_1 = data_1[data_1.duplicated(subset='id', keep=False)]  # keep=False помечает все дубликаты, включая их первое вхождение
dupl_data_1_sorted = dupl_data_1.sort_values('id')

# выведем полученные неявные дубликаты для анализа
print(f"Всего строк с дубликатами в столбце 'id': {len(dupl_data_1)}")
print("Строки с дубликатами:")
print(dupl_data_1_sorted)


# **Примечание:** Как показала проверка, явные дубликаты отсутствуют. Однако встречаются скважины с одинаковыым значением в столбце 'id', но разными значениями в других столбцах, скорее всего, это может быть связано с ошибкой присвоения уникального индентификатора скважен. Данные с одинаковыми значениями в столбце 'id' удалять не будем.

# ### Предобработка данных в таблице "data_2"

# In[20]:


# посмотрим количество пропусков в данных
data_2.isna().sum()


# **Примечание:** Как и было замечено ранее, пропуски в данных отсутствуют.

# In[21]:


# проверим датафрейм на наличие явных дубликатов
data_2.duplicated().sum()


# **Примечание:** Явные дубликаты в данных отсутствуют.

# In[22]:


# проверим датафрейм на наличие неявных дубликатов
# проверка будет проводиться по столбцу 'id'
data_2.duplicated(subset='id').sum()


# In[23]:


# проверим датафрейм на наличие неявных дубликатов
# проверка будет проводиться по столбцу 'id'
dupl_data_2 = data_2[data_2.duplicated(subset='id', keep=False)]  # keep=False помечает все дубликаты, включая их первое вхождение
dupl_data_2_sorted = dupl_data_2.sort_values('id')

# выведем полученные неявные дубликаты для анализа
print(f"Всего строк с дубликатами в столбце 'id': {len(dupl_data_2)}")
print("Строки с дубликатами:")
print(dupl_data_2_sorted)


# **Примечание:** Как показала проверка, явные дубликаты отсутствуют. Однако встречаются скважины с одинаковыым значением в столбце 'id', но разными значениями в других столбцах, скорее всего, это может быть связано с ошибкой присвоения уникального индентификатора скважен. Данные с одинаковыми значениями в столбце 'id' удалять не будем.

# **Обобщающий вывод этапа "Предобработка данных":**
# 1. Во всех таблицах отсутствуют пропуски.
# 2. В таблицах отсутствуют явные и неявные дубликаты.
# 3. Во всех таблицах встречаются скважины с одинаковыым значением в столбце 'id', но разными значениями в других столбцах, скорее всего, это может быть связано **с ошибкой присвоения уникального индентификатора скважен**. Данные с одинаковыми значениями в столбце 'id' удалять не будем.

# ## Исследовательский анализ данных

# ### Исследовательский анализ данных таблицы 'data_0'

# In[24]:


# проведем статистический анализ всех признаков в таблице 'data_0'
# все столбцы, кроме столбца 'id', имеют количественные значения
print('Описательная статистика для количественных признаков:')
data_0.describe(include=[np.number])


# In[25]:


# напишем собственную функцию для визуализации всех количественных признаков на одном графике
def plot_histograms(data, title):
    # устанавливаем параметры графика
    plt.figure(figsize=(12, 8))
    
    # строим гистограммы для каждого признака
    axes = data.iloc[:, 1:].hist(
        figsize=(12, 8),
        bins=30,
        color='steelblue',
        edgecolor='white',
        grid=False, 
        layout=(2, 2), 
)
    
    # окончательно настраиваем график
    plt.suptitle(title, fontsize=12)
    plt.tight_layout()
    plt.show()


# In[26]:


# визуализируем количественные признаки таблицы 'data_0'
plot_histograms(data_0, "Распределение признаков 'data_0'")


# In[27]:


# напишем собственную функцию для построения графика "Ящик с усами" для всех количественных признаков
# устанавливаем параметры графика
def plot_boxplots(data, title):
    # устанавливаем параметры графика
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
    
    # строим график для первых трех признаков
    sns.boxplot(data=data.iloc[:,1:4], ax=ax1)
    ax1.set_xticklabels(ax1.get_xticklabels())
    ax1.set_title("График 'Ящик с усами' для признаков 'f0', 'f1', 'f2'", fontsize=10, pad=10)
    
    # строим график для признака объем запасов в скважине
    sns.boxplot(data=data.iloc[:,4:], ax=ax2)
    ax2.set_xticklabels(ax2.get_xticklabels())
    ax2.set_title("График 'Ящик с усами' для признака 'product'", fontsize=10, pad=10)

    # окончательно настраиваем график
    plt.suptitle(title, fontsize=12)
    plt.tight_layout()
    plt.show()


# In[28]:


# визуализируем количественные признаки таблицы 'data_0'
plot_boxplots(data_0, "График 'Ящик с усами' для 'data_0'")


# ### Исследовательский анализ данных таблицы 'data_1'

# In[29]:


# проведем статистический анализ всех признаков в таблице 'data_1'
# все столбцы, кроме столбца 'id', имеют количественные значения
print('Описательная статистика для количественных признаков:')
data_1.describe(include=[np.number])


# In[30]:


# визуализируем количественные признаки таблицы 'data_1'
plot_histograms(data_1, "Распределение признаков 'data_1'")


# In[31]:


# визуализируем количественные признаки таблицы 'data_1'
plot_boxplots(data_1, "График 'Ящик с усами' для 'data_1'")


# ### Исследовательский анализ данных таблицы 'data_2'

# In[32]:


# проведем статистический анализ всех признаков в таблице 'data_2'
# все столбцы, кроме столбца 'id', имеют количественные значения
print('Описательная статистика для количественных признаков:')
data_2.describe(include=[np.number])


# In[33]:


# визуализируем количественные признаки таблицы 'data_2'
plot_histograms(data_2, "Распределение признаков 'data_2'")


# In[34]:


# визуализируем количественные признаки таблицы 'data_2'
plot_boxplots(data_2, "График 'Ящик с усами' для 'data_2'")


# **Обощающий вывод этапа "Исследовательский анализ данных":**
# 
# Трудно говорить конкретно о характере распределения данных, так как почти ничего неизвестно о их природе, исходные данные **синтетические**. 
# 
# Изначальная информация такова: f0, f1, f2 — три признака точек **(неважно, что они означают, но сами признаки значимы)**;
# product — объём запасов в скважине (тыс. баррелей).
# 
# Поэтому можно сказать общие сведения, которые были получены в процессе проведения исследовательского нализа данных:
# 1. **Для таблицы 'data_0':** Признак 'f2' имеет нормальное распределение, но при этом имеет достаточно большое количество выбросов, которые мы не будем удалять из исследования, так как не знаем, о чем они говорят. Остальные признаки выбросов не имеют.
# 2. **Для таблицы 'data_1':** Признак 'f1' имеет нормальное распределение, но при этом имеет достаточно большое количество выбросов, которые мы не будем удалять из исследования, так как не знаем, о чем они говорят. Остальные признаки выбросов не имеют.
# 3. **Для таблицы 'data_2':** Признаки 'f0', 'f1' и 'f2' имеют нормальное распределение, но при этом имеют достаточно большое количество выбросов, которые мы не будем удалять из исследования, так как не знаем, о чем они говорят. Признак 'product' выбросов не имеет.

# ## Проведение корреляционного анализа данных

# In[35]:


# посмотрим наличие корреляции между признаками таблицы 'data_0'
matrix_0 = data_0.drop('id', axis=1)
matrix_0.corr()


# In[36]:


# посмотрим наличие корреляции между признаками таблицы 'data_1'
matrix_1 = data_1.drop('id', axis=1)
matrix_1.corr()


# In[37]:


# посмотрим наличие корреляции между признаками таблицы 'data_2'
matrix_2 = data_2.drop('id', axis=1)
matrix_2.corr()


# **Обобщающий вывод этапа "Проведение корреляционного анализа данных":**
# 
# Среди всех признаков больше всего выделяется признак 'f2', который имеет корреляцию выше остальных признаков с признаком 'product'. В таблице 'data_1' прослеживается особенно сильная корреляция между признаками 'f2' и 'product', равная 0.99.

# ## Обучение и проверка модели

# ### Разобьем данные на обучающую и валидационную выборки

# In[38]:


# напишем собственную функцию для разбиения
def split_data(data):
    data_target = data['product']
    data_features = data.drop(['id', 'product'], axis=1)
    features_train, features_valid, target_train, target_valid = train_test_split(data_features, data_target, test_size=0.25, random_state=RANDOM_STATE, shuffle = True)
    return features_train, features_valid, target_train, target_valid


# In[39]:


# разбиваем все три датасета на обучающую и валидационную выборки
# для таблицы 'data_0'
features_train_0, features_valid_0, target_train_0, target_valid_0 = split_data(data_0)
# для таблицы 'data_1'
features_train_1, features_valid_1, target_train_1, target_valid_1 = split_data(data_1)
# для таблицы 'data_2'
features_train_2, features_valid_2, target_train_2, target_valid_2 = split_data(data_2)


# ### Обучение модели

# In[40]:


# по условиям задачи для обучения модели подходит только линейная регрессия
# напишем собственную функцию для обучения модели
def learn_model(features_train, features_valid, target_train, target_valid):
    model = LinearRegression()
    model.fit(features_train, target_train)
    predictions = model.predict(features_valid)
    predictions_constant = pd.Series(target_train.mean(), index=target_valid.index)

    # расчет метрик
    rmse = mean_squared_error(target_valid, predictions) ** 0.5
    rmse_constant = mean_squared_error(target_valid, predictions_constant)**0.5
    mean_product = predictions.mean()
    return target_valid, predictions, rmse, rmse_constant, mean_product


# In[41]:


def model_lr(future_train, future_valid, target_train, target_valid):
    lr.fit(future_train, target_train)
    predicted_valid = lr.predict(future_valid)
    
    mse = mean_squared_error(target_valid, predicted_valid)
    rmse = mean_squared_error(target_valid, predicted_valid)**0.5
    mean_sum = predicted_valid.sum()/len(predicted_valid)
    return mse, rmse, mean_sum, predicted_valid


# #### Обучение модели для региона 'data_0'

# In[42]:


# полученные результаты для первого региона 'data_0'
data_0_target_valid, data_0_predictions_valid, data_0_rmse, data_0_rmse_constant, data_0_mean_product = learn_model(features_train_0, features_valid_0, target_train_0, target_valid_0)

# выведем предсказания и правильные ответы на валидационной выборке
data_0_predictions_valid = pd.Series(data_0_predictions_valid, index=data_0_target_valid.index)
print(data_0_target_valid.head())
print(data_0_predictions_valid.head())


# In[43]:


# выведем рассчитанные метрики для 'data_0'
print(f'Регион data_0: Метрика RMSE модели составляет {data_0_rmse:.2f}, Метрика RMSE модели-константы составляет {data_0_rmse_constant:.2f}, Средний запас предсказанного сырья {data_0_mean_product:.2f}')


# #### Обучение модели для региона 'data_1'

# In[44]:


# полученные результаты для первого региона 'data_1'
data_1_target_valid, data_1_predictions_valid, data_1_rmse, data_1_rmse_constant, data_1_mean_product = learn_model(features_train_1, features_valid_1, target_train_1, target_valid_1)

# выведем предсказания и правильные ответы на валидационной выборке
data_1_predictions_valid = pd.Series(data_1_predictions_valid, index=data_1_target_valid.index)
print(data_1_target_valid.head())
print(data_1_predictions_valid.head())


# In[45]:


# выведем рассчитанные метрики для 'data_1'
print(f'Регион data_1: Метрика RMSE модели составляет {data_1_rmse:.2f}, Метрика RMSE модели-константы составляет {data_1_rmse_constant:.2f}, Средний запас предсказанного сырья {data_1_mean_product:.2f}')


# #### Обучение модели для региона 'data_2'

# In[46]:


# полученные результаты для первого региона 'data_2'
data_2_target_valid, data_2_predictions_valid, data_2_rmse, data_2_rmse_constant, data_2_mean_product = learn_model(features_train_2, features_valid_2, target_train_2, target_valid_2)

# выведем предсказания и правильные ответы на валидационной выборке
data_2_predictions_valid = pd.Series(data_2_predictions_valid, index=data_2_target_valid.index)
print(data_2_target_valid.head())
print(data_2_predictions_valid.head())


# In[47]:


# выведем рассчитанные метрики для 'data_2'
print(f'Регион data_2: Метрика RMSE модели составляет {data_2_rmse:.2f}, Метрика RMSE модели-константы составляет {data_2_rmse_constant:.2f}, Средний запас предсказанного сырья {data_2_mean_product:.2f}')


# **Обобщающий вывод этапа "обучение и проверка модели":**
# 
# Наиболее точная по предсказанию модель оказалась для региона 'data_1', метрика RMSE модели для этого региона равна 0.89, что является лучшим результатом по сравнению с другими моделями. Однако средний запас предсказанного сырья для региона 'data_1' составил 68.71, что значительно меньше по сравнению с другими регионами. Наибольший средний запас предсказанного сырья показал регион 'data_2', равный 94.77.

# ## Подготовка к расчету прибыли

# In[48]:


# все ключевые значения для расчета прибыли сохранены в отдельных переменных в начале проекта
# рассчитаем достаточный объём сырья для безубыточной разработки новой скважины

# рассчитаем бюджет для одной скважины
budget_well = BUDGET / K
# рассчитаем объем сырья для безубыточной разработки скважины
product_well = budget_well / BAR_COST
print(f'Достаточный объём сырья для безубыточной разработки новой скважины: {product_well:.2f} баррель.')


# In[49]:


# сравним полученный достаточный объем сырья со средним запасом сырья в каждом регионе
# т.к. мы уже знаем, что средний запас сырья в каждом регионе меньше рассчитанного объема, то посмотрим,  насколько
print(f'Объём сырья для безубыточной разработки новой скважины в data_0: {product_well- data_0_mean_product:.2f} баррель.')
print(f'Объём сырья для безубыточной разработки новой скважины в data_1: {product_well - data_1_mean_product:.2f} баррель.')
print(f'Объём сырья для безубыточной разработки новой скважины в data_2: {product_well - data_2_mean_product:.2f} баррель.')


# **Обобщающий вывод этапа "Подготовка к расчету прибыли":**
# 1. Достаточный объем сырья для безубыточной разработки новой скважины равен 111.11 баррель.
# 2. Средний запас сырья в каждом регионе меньше достаточного объема сырья для безубаточной разработки новой скважины на 18.71, 42.20 и 16.34 баррель соответственно.

# ## Расчет прибыли и рисков

# ### Написание функции для расчета прибыли по выбранным скважинам и предсказаниям модели

# In[52]:


# напишем собственную функцию для расчета прибыли
def revenue(target, predictions, count):
    target = target.reset_index(drop=True)
    predictions = predictions.reset_index(drop=True)
    preds_sorted = predictions.sort_values(ascending=False)
    select = target[preds_sorted.index][:count]
    return BAR_COST * select.sum() - BUDGET


# In[54]:


# напишем собственную функцию для применения техники Bootstrap
def bootstrap(target, predictions):
    state = np.random.RandomState(12345)
    values = []

    for i in range(1000):
        subsample_target = target.sample(n=N, replace=True, random_state=state)
        subsample_predictions = predictions[subsample_target.index]
        values.append(revenue(subsample_target, subsample_predictions, K))

    values = pd.Series(values)
    values_mean = round(values.mean(), 2)
    lower = values.quantile(0.025) 
    upper = values.quantile(0.975)
    risk = int(len(values[values < 0]) / len(values) * 100)
    return values_mean, lower, upper, risk


# ### Расчет прибыли и рисков для каждого региона

# #### Расчет прибыли и рисков для региона 'data_0'

# In[55]:


# вызовем собственную функцию для региона 'data_0'
data_0_values_mean, data_0_lower, data_0_upper, data_0_risk = bootstrap(data_0_target_valid, data_0_predictions_valid)
print('Средняя прибыль от месторождений в регионе "data_0":', data_0_values_mean)
print(f'95% доверительный интервал для средней прибыли 200 лучших месторождений в регионе "data_0": {round(data_0_lower,2)} - {round(data_0_upper,2)}')
print(f'Риск убытков в регионе "data_0": {data_0_risk}%')


# #### Расчет прибыли и рисков для региона 'data_1'

# In[56]:


# вызовем собственную функцию для региона 'data_1'
data_1_values_mean, data_1_lower, data_1_upper, data_1_risk = bootstrap(data_1_target_valid, data_1_predictions_valid)
print('Средняя прибыль от месторождений в регионе "data_1":', data_1_values_mean)
print(f'95% доверительный интервал для средней прибыли 200 лучших месторождений в регионе "data_1": {round(data_1_lower,2)} - {round(data_1_upper,2)}')
print(f'Риск убытков в регионе "data_1": {data_1_risk}%')


# #### Расчет прибыли и рисков для региона 'data_2'

# In[57]:


# вызовем собственную функцию для региона 'data_2'
data_2_values_mean, data_2_lower, data_2_upper, data_2_risk = bootstrap(data_2_target_valid, data_2_predictions_valid)
print('Средняя прибыль от месторождений в регионе "data_2":', data_2_values_mean)
print(f'95% доверительный интервал для средней прибыли 200 лучших месторождений в регионе "data_2": {round(data_2_lower,2)} - {round(data_2_upper,2)}')
print(f'Риск убытков в регионе "data_2": {data_2_risk}%')


# **Обобщающий вывод этапа "Расчет прибыли и рисков":**
# 
# По полученным результатам стало видно, что добыча в регионе 'data_1' принесет прибыли больше, чем другии регионы. Ее показатели значительно лучше других и составили: Средняя прибыль от месторождений в ркгионе: 432624131.81 у.е. (самая высокая по сравнению с другими), Риск убытков 1% (самый низкий по сравнению с другими) и самый низкий разброс в доверительном интервале.

# ## Итоговые выводы

# В ходе работы были проанализированы данные, предоставленные добывающей компанией «ГлавРосГосНефть», которая хочет разработать модель, позволяющую определить регион, где добыча принесет наибольшую прибыль.
# 
# Для реализации этой задачи было выполнено 7 этапов работы.
# 
# На **1 этапе: Загрузка и изучение данных** были проанализированы все исходные данные с входными признаками, дале на **2 этапе: Предобработка данных** данные были изучены более детально, а именно:
# 1. Во всех таблицах отсутствуют пропуски.
# 2. В таблицах отсутствуют явные и неявные дубликаты.
# 3. Во всех таблицах встречаются скважины с одинаковыым значением в столбце 'id', но разными значениями в других столбцах, скорее всего, это может быть связано **с ошибкой присвоения уникального индентификатора скважен**. Данные с одинаковыми значениями в столбце 'id' удалять не будем.
# 
# На **3 этапе: Исследовательский анализ данных** были получены следующие результаты, а именно:
# 
# Трудно говорить конкретно о характере распределения данных, так как почти ничего неизвестно о их природе, исходные данные **синтетические**. 
# 
# Изначальная информация такова: f0, f1, f2 — три признака точек **(неважно, что они означают, но сами признаки значимы)**;
# product — объём запасов в скважине (тыс. баррелей).
# 
# Поэтому можно сказать общие сведения, которые были получены в процессе проведения исследовательского нализа данных:
# 1. **Для таблицы 'data_0':** Признак 'f2' имеет нормальное распределение, но при этом имеет достаточно большое количество выбросов, которые мы не будем удалять из исследования, так как не знаем, о чем они говорят. Остальные признаки выбросов не имеют.
# 2. **Для таблицы 'data_1':** Признак 'f1' имеет нормальное распределение, но при этом имеет достаточно большое количество выбросов, которые мы не будем удалять из исследования, так как не знаем, о чем они говорят. Остальные признаки выбросов не имеют.
# 3. **Для таблицы 'data_2':** Признаки 'f0', 'f1' и 'f2' имеют нормальное распределение, но при этом имеют достаточно большое количество выбросов, которые мы не будем удалять из исследования, так как не знаем, о чем они говорят. Признак 'product' выбросов не имеет.
# 
# На **4 этапе: Проведение корреляционного анализа данных** были получены следующие результаты:
# 
# Среди всех признаков больше всего выделяется признак 'f2', который имеет корреляцию выше остальных признаков с признаком 'product'. В таблице 'data_1' прослеживается особенно сильная корреляция между признаками 'f2' и 'product', равная 0.99.
# 
# На **5 этапе: Обучение моделей** были получены следующие результаты:
# 
# Наиболее точная по предсказанию модель оказалась для региона 'data_1', метрика RMSE модели для этого региона равна 0.89, что является лучшим результатом по сравнению с другими моделями. Однако средний запас предсказанного сырья для региона 'data_1' составил 68.71, что значительно меньше по сравнению с другими регионами. Наибольший средний запас предсказанного сырья показал регион 'data_2', равный 94.77.
# 
# На **6 этапе: Подготовка к расчету прибыли** были рассчитано, что:
# 1. Достаточный объем сырья для безубыточной разработки новой скважины равен 111.11 баррель.
# 2. Средний запас сырья в каждом регионе меньше достаточного объема сырья для безубаточной разработки новой скважины на 18.71, 42.20 и 16.34 баррель соответственно.
# 
# На **7 этапе: Расчет прибыли и рисков** было получено, что:
# 
# Добыча в регионе 'data_1' принесет прибыли больше, чем другии регионы. Ее показатели значительно лучше других и составили: Средняя прибыль от месторождений в ркгионе: 432624131.81 у.е. (самая высокая по сравнению с другими), Риск убытков 1% (самый низкий по сравнению с другими) и самый низкий разброс в доверительном интервале.
# 
# **Таким образом,** анализирую всю полученную в ходе исследования информацию, можно сделать вывод, что наилучшим регионом, где добыча принесет наибольшую прибыль, стал 'data_1'.
